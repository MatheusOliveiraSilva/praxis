# Streaming vs Reasoning: ComparaÃ§Ã£o Visual

## ğŸ“Š **DiferenÃ§as Fundamentais**

### **GPT-4 (Streaming Habilitado)**

```
Cliente faz request:
POST /v1/chat/completions
{
  "model": "gpt-4",
  "messages": [...],
  "stream": true
}

Servidor responde (SSE):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Chunk 1 (t=0ms)                         â”‚
â”‚ delta: { role: "assistant", content: ""}â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Chunk 2 (t=20ms)                        â”‚
â”‚ delta: { content: "The" }               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Chunk 3 (t=40ms)                        â”‚
â”‚ delta: { content: " answer" }           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Chunk 4 (t=60ms)                        â”‚
â”‚ delta: { content: " is" }               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Chunk 5 (t=80ms)                        â”‚
â”‚ delta: { content: " 42" }               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Chunk 6 (t=100ms)                       â”‚
â”‚ delta: {}                               â”‚
â”‚ finish_reason: "stop"                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Total: 100ms
UX: UsuÃ¡rio vÃª texto aparecer em tempo real
```

---

### **o1-preview (Reasoning, Sem Streaming)**

```
Cliente faz request:
POST /v1/chat/completions
{
  "model": "o1-preview",
  "messages": [...]
  // NÃƒO pode usar "stream": true
}

Servidor responde (JSON completo):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                         â”‚
â”‚ Aguardando... (pode demorar 10-30s)    â”‚
â”‚                                         â”‚
â”‚         [modelo estÃ¡ pensando]          â”‚
â”‚                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Resposta (t=15000ms)                    â”‚
â”‚                                         â”‚
â”‚ {                                       â”‚
â”‚   "choices": [{                         â”‚
â”‚     "message": {                        â”‚
â”‚       "role": "assistant",              â”‚
â”‚       "reasoning_content": "Let me...  â”‚
â”‚         ...think step by step...",      â”‚
â”‚       "content": "The answer is 42."    â”‚
â”‚     }                                   â”‚
â”‚   }],                                   â”‚
â”‚   "usage": {                            â”‚
â”‚     "reasoning_tokens": 450             â”‚
â”‚   }                                     â”‚
â”‚ }                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Total: 15000ms (tudo de uma vez)
UX: UsuÃ¡rio espera, depois vÃª tudo junto
```

---

## ğŸ”„ **Como Nossa Arquitetura Praxis Espera**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Praxis StreamEvent (independente do LLM)â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Reasoning { "Let" }                      â”‚
â”‚ Reasoning { " me" }                      â”‚
â”‚ Reasoning { " think" }                   â”‚
â”‚ Reasoning { "..." }                      â”‚
â”‚ Message { "The" }                        â”‚
â”‚ Message { " answer" }                    â”‚
â”‚ Message { " is" }                        â”‚
â”‚ Message { " 42" }                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Sempre token-by-token, independente do provider!
```

---

## ğŸ¯ **Problema: Incompatibilidade**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   GPT-4     â”‚              â”‚  o1-preview â”‚
â”‚             â”‚              â”‚             â”‚
â”‚ âœ… Streaming â”‚              â”‚ âŒ Streaming â”‚
â”‚ âŒ Reasoning â”‚              â”‚ âœ… Reasoning â”‚
â”‚             â”‚              â”‚             â”‚
â”‚ delta.      â”‚              â”‚ message.    â”‚
â”‚  content    â”‚              â”‚  reasoning_ â”‚
â”‚   (chunks)  â”‚              â”‚  content    â”‚
â”‚             â”‚              â”‚  (completo) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                            â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   Praxis Adapter      â”‚
         â”‚   (precisa unificar)  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ StreamEvent::Reasoningâ”‚
         â”‚ StreamEvent::Message  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’¡ **SoluÃ§Ã£o: Adapter Pattern**

### **Para GPT-4 (jÃ¡ funciona)**

```rust
// Streaming nativo
let mut stream = client.chat_completion_stream(...).await?;

while let Some(chunk) = stream.next().await {
    if let Some(content) = chunk.content() {
        // Emitir como Message (nÃ£o hÃ¡ reasoning)
        emit(StreamEvent::Message { content });
    }
}
```

### **Para o1 (precisa implementar)**

```rust
// NÃ£o-streaming, simular chunks
let response = client.chat_completion(...).await?;

// 1. Processar reasoning
if let Some(reasoning) = response.reasoning() {
    // OpÃ§Ã£o A: Enviar tudo de uma vez
    emit(StreamEvent::Reasoning { content: reasoning });
    
    // OpÃ§Ã£o B: Simular streaming (dividir em palavras)
    for word in reasoning.split_whitespace() {
        emit(StreamEvent::Reasoning { content: word });
        tokio::time::sleep(Duration::from_millis(50)).await;
    }
}

// 2. Processar message
if let Some(content) = response.content() {
    // Mesmo processo
    emit(StreamEvent::Message { content });
}
```

---

## ğŸ—ï¸ **Arquitetura Proposta**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              LLMClient Trait                    â”‚
â”‚  (abstraÃ§Ã£o Praxis, provider-agnostic)          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  async fn stream(...)                           â”‚
â”‚    -> Stream<StreamEvent>                       â”‚
â”‚                                                 â”‚
â”‚  // Sempre retorna StreamEvent!                â”‚
â”‚  // Adapter faz a conversÃ£o                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GPT4Adapter   â”‚       â”‚   O1Adapter     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ - Usa streamingâ”‚       â”‚ - NÃ£o usa streamâ”‚
â”‚   nativo       â”‚       â”‚ - Simula chunks â”‚
â”‚ - delta.contentâ”‚       â”‚ - reasoning_    â”‚
â”‚   â†’ Message    â”‚       â”‚   content       â”‚
â”‚                â”‚       â”‚   â†’ Reasoning   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ **Trade-offs**

### **OpÃ§Ã£o 1: Simular Streaming para o1**

**PrÃ³s:**
- âœ… UX consistente
- âœ… Praxis nÃ£o precisa saber qual modelo estÃ¡ usando
- âœ… Frontend Ãºnico

**Contras:**
- âŒ LatÃªncia inicial (espera resposta completa)
- âŒ "Fake" streaming (nÃ£o Ã© real)
- âŒ Mais complexo

### **OpÃ§Ã£o 2: Enviar Blocos Ãšnicos para o1**

**PrÃ³s:**
- âœ… Simples
- âœ… Honesto (nÃ£o finge streaming)
- âœ… Menos cÃ³digo

**Contras:**
- âŒ UX diferente (frontend precisa lidar com 2 casos)
- âŒ Praxis sabe detalhes do provider

---

## ğŸ¬ **Exemplo de ImplementaÃ§Ã£o**

### **Estrutura Unificada**

```rust
pub enum LLMResponse {
    Streaming(Pin<Box<dyn Stream<Item = StreamEvent>>>),
    Complete {
        reasoning: Option<String>,
        content: String,
        tool_calls: Option<Vec<ToolCall>>,
    },
}

impl OpenAIClient {
    pub async fn execute(&self, model: &str, ...) -> Result<LLMResponse> {
        match model {
            "gpt-4" | "gpt-3.5-turbo" => {
                // Streaming real
                let stream = self.chat_completion_stream(...).await?;
                Ok(LLMResponse::Streaming(stream))
            }
            "o1-preview" | "o1-mini" => {
                // NÃ£o-streaming
                let response = self.chat_completion(...).await?;
                Ok(LLMResponse::Complete {
                    reasoning: response.reasoning().map(String::from),
                    content: response.content().unwrap().to_string(),
                    tool_calls: response.tool_calls().map(|t| t.to_vec()),
                })
            }
            _ => Err(anyhow!("Unsupported model"))
        }
    }
}
```

---

## âœ… **ConclusÃ£o**

**Nossa implementaÃ§Ã£o atual:**
- âœ… Streaming funciona perfeitamente para GPT-4
- âŒ **NÃƒO captura reasoning do o1**
- âŒ **NÃƒO tem campos necessÃ¡rios**

**Precisamos:**
1. Adicionar `reasoning_content` e `reasoning_tokens`
2. Criar adapter para o1 (simular streaming ou enviar blocos)
3. Decidir estratÃ©gia de UX (streaming fake vs blocos completos)

**RecomendaÃ§Ã£o:**
- **Curto prazo:** Adicionar campos, enviar blocos completos
- **Longo prazo:** Implementar streaming simulado para UX consistente
