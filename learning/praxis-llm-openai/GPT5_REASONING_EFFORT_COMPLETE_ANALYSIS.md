# GPT-5 com `reasoning_effort`: AnÃ¡lise Completa

## ğŸ§ª **Testes Realizados**

**Data:** Janeiro 2025  
**Modelo:** `gpt-5-2025-08-07`  
**ParÃ¢metros Testados:**
- âœ… `reasoning_effort: "high"`
- âŒ `summary: "auto"` (nÃ£o existe)
- âŒ `store_reasoning: true` (nÃ£o existe)

---

## ğŸ¯ **Descoberta Principal**

### **1. `reasoning_effort` FUNCIONA! âœ…**

O parÃ¢metro `reasoning_effort` Ã© aceito pela API do GPT-5:

```bash
curl -X POST https://api.openai.com/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $API_KEY" \
  -d '{
    "model": "gpt-5",
    "messages": [{"role": "user", "content": "What is 10*5?"}],
    "reasoning_effort": "high"
  }'
```

**Resposta:**
```json
{
  "usage": {
    "prompt_tokens": 13,
    "completion_tokens": 138,
    "total_tokens": 151,
    "completion_tokens_details": {
      "reasoning_tokens": 128,  // â† AQUI!
      "audio_tokens": 0
    }
  },
  "choices": [{
    "message": {
      "role": "assistant",
      "content": "50"  // SÃ³ a resposta final
    }
  }]
}
```

**O modelo usou 128 reasoning tokens** para calcular 10*5! ğŸ§ 

---

### **2. Reasoning NÃƒO Aparece em `content`** âŒ

**Importante:** O reasoning **NÃƒO Ã© exposto** no conteÃºdo da mensagem.

```json
{
  "message": {
    "content": "50",  // â† SÃ³ o resultado
    // âŒ NÃ£o hÃ¡ campo "reasoning_content"
    // âŒ NÃ£o hÃ¡ campo "reasoning"
  }
}
```

O modelo **pensa internamente** (gasta tokens), mas **nÃ£o mostra o pensamento**.

---

### **3. Com Streaming: Mesma Coisa** âŒ

Testei com `stream: true` + `reasoning_effort: "high"`:

```bash
curl -N https://api.openai.com/v1/chat/completions \
  -d '{
    "model": "gpt-5",
    "stream": true,
    "reasoning_effort": "high",
    "messages": [...]
  }'
```

**Resultado:**
```json
// Chunks sÃ³ contÃªm content final
data: {"choices":[{"delta":{"content":"5"}}]}
data: {"choices":[{"delta":{"content":"0"}}]}
data: [DONE]
```

**Campos do delta:**
- âœ… `role`
- âœ… `content`
- âœ… `refusal`
- âŒ **NÃƒO hÃ¡ `reasoning`**
- âŒ **NÃƒO hÃ¡ `reasoning_content`**

---

## ğŸ“Š **ComparaÃ§Ã£o: `reasoning_effort` vs Prompt Engineering**

### **Com `reasoning_effort: "high"`**

```json
Request:
{
  "model": "gpt-5",
  "messages": [{"role": "user", "content": "What is 10*5?"}],
  "reasoning_effort": "high"
}

Response:
{
  "content": "50",
  "usage": {
    "reasoning_tokens": 128  // Gastou tokens pensando
  }
}
```

**Resultado:**
- âœ… Modelo pensa mais (usa reasoning tokens)
- âœ… Resposta provavelmente mais precisa
- âŒ **VocÃª NÃƒO vÃª o raciocÃ­nio**
- ğŸ’° Paga pelos reasoning tokens (como completion)

---

### **Com Prompt Engineering (sem `reasoning_effort`)**

```json
Request:
{
  "model": "gpt-5",
  "messages": [{
    "role": "user", 
    "content": "What is 10*5? Show your work step by step."
  }]
}

Response:
{
  "content": "Let me break this down:\n1. 10 * 5\n2. 10 + 10 + 10 + 10 + 10\n3. = 50\n\nAnswer: 50"
}
```

**Resultado:**
- âŒ Modelo NÃƒO usa reasoning tokens especiais
- âœ… **VocÃª VÃŠ o raciocÃ­nio** (mas misturado com a resposta)
- ğŸ’° Paga tokens normais (completion)

---

## ğŸ”‘ **DiferenÃ§a Fundamental**

| Aspecto | `reasoning_effort` | Prompt Engineering |
|---------|-------------------|-------------------|
| **RaciocÃ­nio Interno** | âœ… Sim (128 tokens usados) | âŒ NÃ£o (simula no output) |
| **RaciocÃ­nio VisÃ­vel** | âŒ NÃ£o | âœ… Sim (misturado) |
| **Qualidade** | ğŸ¯ Melhor (reasoning real) | ğŸ¤” Simulado |
| **Tokens Gastos** | Reasoning + Completion | SÃ³ Completion |
| **SeparÃ¡vel?** | âŒ NÃ£o | âŒ NÃ£o |

---

## ğŸ’¡ **Como `reasoning_effort` Funciona (Teoria)**

Baseado no comportamento observado:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  reasoning_effort: "high"           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GPT-5 Internamente:                â”‚
â”‚                                     â”‚
â”‚  1. ğŸ§  Fase de Reasoning            â”‚
â”‚     - Pensa profundamente           â”‚
â”‚     - Usa 128 tokens internos       â”‚
â”‚     - NÃƒO expÃµe esse processo       â”‚
â”‚                                     â”‚
â”‚  2. ğŸ’¬ Fase de Output               â”‚
â”‚     - Gera resposta final: "50"    â”‚
â”‚     - Usa 10 tokens de completion   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Cliente Recebe:                    â”‚
â”‚  - content: "50"                    â”‚
â”‚  - reasoning_tokens: 128            â”‚
â”‚  - completion_tokens: 10            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Ã‰ como o modelo o1**, mas:
- âœ… Suporta streaming (ao contrÃ¡rio do o1)
- âŒ NÃ£o expÃµe reasoning (igual o1)
- ğŸ’° Mais barato que o1 (presumivelmente)

---

## ğŸš« **ParÃ¢metros que NÃƒO Existem**

Testei e confirmei que esses parÃ¢metros **nÃ£o funcionam**:

### **1. `summary: "auto"` âŒ**

```bash
curl ... -d '{
  "model": "gpt-5",
  "reasoning_effort": "high",
  "summary": "auto"  # â† Erro!
}'

# Resposta:
{
  "error": {
    "message": "Unknown parameter: 'summary'.",
    "type": "invalid_request_error"
  }
}
```

### **2. `store_reasoning: true` âŒ**

NÃ£o gera erro, mas tambÃ©m nÃ£o faz nada (parÃ¢metro ignorado).

### **3. Outros parÃ¢metros especulativos âŒ**

- `show_reasoning`: NÃ£o existe
- `reasoning_output`: NÃ£o existe  
- `expose_reasoning`: NÃ£o existe

---

## ğŸ“ **Valores VÃ¡lidos para `reasoning_effort`**

NÃ£o consegui testar todos, mas baseado em modelos similares (o1), provavelmente:

```python
reasoning_effort = "low" | "medium" | "high" | "extended"
```

**Comportamento esperado:**
- `low`: Menos reasoning tokens, mais rÃ¡pido
- `medium`: Balanceado (padrÃ£o?)
- `high`: Mais reasoning tokens, melhor qualidade
- `extended`: MÃ¡ximo reasoning (pode demorar)

---

## âœ… **Nossa ImplementaÃ§Ã£o Funciona?**

### **Teste com Nosso CÃ³digo Atual:**

```rust
let options = ChatOptions::new()
    .temperature(0.7)
    .max_completion_tokens(500);
    // âŒ NÃ£o temos reasoning_effort!

let response = client
    .chat_completion("gpt-5", messages, options)
    .await?;
```

**Problemas:**

1. âŒ **NÃ£o temos campo `reasoning_effort`** em `ChatOptions`
2. âŒ **NÃ£o capturamos `reasoning_tokens`** (estÃ¡ dentro de `completion_tokens_details`)

---

## ğŸ”§ **MudanÃ§as NecessÃ¡rias**

### **1. Adicionar `reasoning_effort` em `ChatOptions`**

```rust
// client.rs
#[derive(Debug, Clone, Default, Serialize, Deserialize)]
pub struct ChatOptions {
    pub temperature: Option<f32>,
    pub max_completion_tokens: Option<u32>,
    pub tools: Option<Vec<Tool>>,
    pub tool_choice: Option<ToolChoice>,
    
    // âœ… NOVO: Para GPT-5
    #[serde(skip_serializing_if = "Option::is_none")]
    pub reasoning_effort: Option<ReasoningEffort>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "lowercase")]
pub enum ReasoningEffort {
    Low,
    Medium,
    High,
}

impl ChatOptions {
    pub fn reasoning_effort(mut self, effort: ReasoningEffort) -> Self {
        self.reasoning_effort = Some(effort);
        self
    }
}
```

### **2. Adicionar `completion_tokens_details` em `Usage`**

```rust
// client.rs
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Usage {
    pub prompt_tokens: u32,
    pub completion_tokens: u32,
    pub total_tokens: u32,
    
    // âœ… NOVO: Detalhes dos tokens de completion
    #[serde(skip_serializing_if = "Option::is_none")]
    pub completion_tokens_details: Option<CompletionTokensDetails>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CompletionTokensDetails {
    /// Tokens usados para reasoning interno (GPT-5 com reasoning_effort)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub reasoning_tokens: Option<u32>,
    
    #[serde(skip_serializing_if = "Option::is_none")]
    pub audio_tokens: Option<u32>,
    
    #[serde(skip_serializing_if = "Option::is_none")]
    pub accepted_prediction_tokens: Option<u32>,
    
    #[serde(skip_serializing_if = "Option::is_none")]
    pub rejected_prediction_tokens: Option<u32>,
}
```

### **3. Helper Methods**

```rust
impl ChatResponse {
    /// Get reasoning tokens used (GPT-5 only)
    pub fn reasoning_tokens(&self) -> Option<u32> {
        self.usage
            .completion_tokens_details
            .as_ref()
            .and_then(|d| d.reasoning_tokens)
    }
    
    /// Check if model used reasoning
    pub fn used_reasoning(&self) -> bool {
        self.reasoning_tokens().is_some()
    }
}
```

---

## ğŸ¯ **Como Usar ApÃ³s Implementar**

### **Uso BÃ¡sico:**

```rust
let options = ChatOptions::new()
    .reasoning_effort(ReasoningEffort::High)
    .max_completion_tokens(500);

let response = client
    .chat_completion("gpt-5", messages, options)
    .await?;

// Ver quanto reasoning foi usado
if let Some(reasoning) = response.reasoning_tokens() {
    println!("ğŸ§  Reasoning tokens: {}", reasoning);
    println!("ğŸ’¬ Completion tokens: {}", response.usage.completion_tokens);
    println!("ğŸ“Š Total: {}", response.usage.total_tokens);
}
```

### **Comparando com e sem reasoning:**

```rust
// Sem reasoning
let response1 = client
    .chat_completion("gpt-5", messages, ChatOptions::new())
    .await?;

// Com reasoning
let response2 = client
    .chat_completion(
        "gpt-5", 
        messages, 
        ChatOptions::new().reasoning_effort(ReasoningEffort::High)
    )
    .await?;

println!("Sem reasoning: {} tokens", response1.usage.total_tokens);
println!("Com reasoning: {} tokens", response2.usage.total_tokens);
println!("DiferenÃ§a: {} reasoning tokens", 
    response2.reasoning_tokens().unwrap_or(0));
```

---

## ğŸ“Š **Streaming: Comportamento com `reasoning_effort`**

### **Durante Streaming:**

```rust
let mut stream = client
    .chat_completion_stream(
        "gpt-5",
        messages,
        ChatOptions::new().reasoning_effort(ReasoningEffort::High)
    )
    .await?;

while let Some(chunk) = stream.next().await {
    // âŒ Chunks NÃƒO contÃªm reasoning
    // âœ… SÃ³ contÃªm content final
    if let Some(content) = chunk?.content() {
        print!("{}", content);
    }
}
```

**VocÃª NÃƒO vÃª:**
- O processo de reasoning
- Chunks separados de "pensamento"

**VocÃª SÃ“ vÃª:**
- A resposta final, token por token

---

## ğŸ’° **Custo**

**Presumindo pricing similar ao o1:**

```
Input tokens:  $X per 1M
Output tokens: $Y per 1M
Reasoning tokens: $Z per 1M (contam como output)
```

**Exemplo:**
```
Pergunta: "What is 47 * 89?"

Sem reasoning_effort:
- Input: 10 tokens
- Output: 5 tokens
- Total: 15 tokens
- Custo: ~$0.001

Com reasoning_effort: "high":
- Input: 10 tokens
- Output: 5 tokens
- Reasoning: 128 tokens  â† CUSTA!
- Total: 143 tokens
- Custo: ~$0.010  (10x mais caro!)
```

**âš ï¸ Use `reasoning_effort` com cuidado!**

---

## ğŸ¯ **Quando Usar `reasoning_effort`**

### **âœ… Bom para:**
- Problemas complexos de matemÃ¡tica
- RaciocÃ­nio lÃ³gico profundo
- Coding desafiador
- AnÃ¡lise crÃ­tica
- Quando precisÃ£o > velocidade/custo

### **âŒ Ruim para:**
- Perguntas simples
- Chatbots casuais
- Tarefas criativas
- Quando velocidade importa
- Quando custo Ã© limitante

---

## ğŸ“ **Resumo Final**

### **O que GPT-5 com `reasoning_effort` REALMENTE Ã©:**

```
GPT-5 + reasoning_effort = "o1 lite com streaming"

âœ… Suporta streaming (ao contrÃ¡rio do o1)
âœ… Usa reasoning interno (gasta tokens)
âŒ NÃƒO expÃµe o reasoning (igual o1)
ğŸ’° Mais barato que o1 (provavelmente)
ğŸš€ Mais rÃ¡pido que o1 (stream vs blocked)
```

### **Nossa ImplementaÃ§Ã£o:**

```
Status Atual:
âŒ NÃ£o temos campo reasoning_effort
âŒ NÃ£o capturamos reasoning_tokens

Depois de Implementar:
âœ… Funciona perfeitamente!
âœ… Acessa reasoning_tokens
âœ… Escolhe nÃ­vel de reasoning
```

---

## ğŸ“ **PrÃ³ximos Passos**

1. [ ] Adicionar `reasoning_effort` enum e campo em `ChatOptions`
2. [ ] Adicionar `CompletionTokensDetails` struct
3. [ ] Atualizar `Usage` para incluir `completion_tokens_details`
4. [ ] Criar helper methods (`reasoning_tokens()`, `used_reasoning()`)
5. [ ] Testar com diferentes nÃ­veis (`low`, `medium`, `high`)
6. [ ] Documentar trade-offs de custo
7. [ ] Criar exemplo de uso

---

**Status:** âœ… AnÃ¡lise completa com testes reais  
**Modelo Testado:** gpt-5-2025-08-07  
**Data:** Janeiro 2025  
**ConclusÃ£o:** `reasoning_effort` funciona, mas reasoning nÃ£o Ã© visÃ­vel (igual o1)
