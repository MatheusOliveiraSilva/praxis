# ğŸš¨ DESCOBERTA CRÃTICA: GPT-5 Responses API

## âš ï¸ **MUDANÃ‡A FUNDAMENTAL NA ARQUITETURA**

GPT-5 com reasoning **NÃƒO usa Chat Completions API tradicional**!

Existe uma **NOVA API** chamada **"Responses API"** especificamente para reasoning models.

---

## ğŸ“Š **Duas APIs Diferentes**

### **1. Chat Completions API (Antiga/Tradicional)**

```bash
POST /v1/chat/completions

# Para GPT-4, GPT-3.5, etc
{
  "model": "gpt-4",
  "messages": [...],
  "temperature": 0.7,
  "max_tokens": 500
}
```

**Funciona com:**
- âœ… GPT-4, GPT-4o
- âœ… GPT-3.5
- âš ï¸ GPT-5 (suportado, mas nÃ£o recomendado para reasoning)

---

### **2. Responses API (Nova!)** ğŸ†•

```bash
POST /v1/responses

# Para GPT-5, o1, modelos de reasoning
{
  "model": "gpt-5",
  "input": [...],              # â† "input" ao invÃ©s de "messages"
  "reasoning": {
    "effort": "medium",        # â† ParÃ¢metro de reasoning
    "summary": "auto"          # â† Pode obter summary!
  },
  "max_output_tokens": 5000    # â† Novo parÃ¢metro
}
```

**Funciona com:**
- âœ… GPT-5, GPT-5-mini, GPT-5-nano
- âœ… o1 (provavelmente o4-mini tambÃ©m)
- âŒ GPT-4 (nÃ£o suporta)

---

## ğŸ¯ **DiferenÃ§as Principais**

| Aspecto | Chat Completions API | Responses API |
|---------|---------------------|---------------|
| **Endpoint** | `/v1/chat/completions` | `/v1/responses` |
| **ParÃ¢metro de Input** | `messages` | `input` |
| **Reasoning** | âŒ NÃ£o exposto | âœ… Com summary! |
| **Max Tokens** | `max_tokens` / `max_completion_tokens` | `max_output_tokens` |
| **Output** | `choices[].message` | `output` array |
| **Reasoning Tokens** | NÃ£o visÃ­vel | `output_tokens_details.reasoning_tokens` |
| **Summary** | âŒ NÃ£o disponÃ­vel | âœ… `reasoning.summary` |

---

## ğŸ” **Responses API: Estrutura Completa**

### **Request:**

```python
from openai import OpenAI
client = OpenAI()

response = client.responses.create(
    model="gpt-5",
    
    # Input (similar a messages)
    input=[
        {"role": "user", "content": "What is 47 * 89?"}
    ],
    
    # âœ… NOVO: ConfiguraÃ§Ã£o de reasoning
    reasoning={
        "effort": "high",     # low | medium | high
        "summary": "auto"     # auto | detailed | concise
    },
    
    # âœ… NOVO: max_output_tokens (inclui reasoning)
    max_output_tokens=5000,
    
    # Opcional: incluir conteÃºdo encriptado
    include=["reasoning.encrypted_content"]
)
```

### **Response:**

```json
{
  "id": "resp_abc123",
  "object": "response",
  "created": 1699999999,
  "model": "gpt-5-2025-08-07",
  
  // âœ… NOVO: Output Ã© array (nÃ£o choices)
  "output": [
    {
      "id": "rs_xyz789",
      "type": "reasoning",  // â† Item de reasoning!
      "summary": [
        {
          "type": "summary_text",
          "text": "I need to multiply 47 by 89. Let me break this down:\n1. 47 * 90 = 4230\n2. 47 * 1 = 47\n3. 4230 - 47 = 4183"
        }
      ]
    },
    {
      "id": "msg_def456",
      "type": "message",  // â† Mensagem final
      "status": "completed",
      "content": [
        {
          "type": "output_text",
          "text": "The answer is 4183."
        }
      ],
      "role": "assistant"
    }
  ],
  
  // âœ… NOVO: Usage com detalhes
  "usage": {
    "input_tokens": 15,
    "output_tokens": 1186,
    "output_tokens_details": {
      "reasoning_tokens": 1024  // â† Aqui!
    },
    "total_tokens": 1201
  },
  
  "status": "completed"  // ou "incomplete"
}
```

---

## ğŸ†• **Recursos da Responses API**

### **1. Reasoning Summary** âœ…

VocÃª **PODE ver o raciocÃ­nio** do modelo (summary, nÃ£o tokens brutos):

```python
response = client.responses.create(
    model="gpt-5",
    input="What is the capital of France?",
    reasoning={
        "effort": "low",
        "summary": "auto"  # â† Pede summary!
    }
)

# Output contÃ©m reasoning item
for item in response.output:
    if item['type'] == 'reasoning':
        print("ğŸ§  Reasoning:", item['summary'][0]['text'])
    elif item['type'] == 'message':
        print("ğŸ’¬ Response:", item['content'][0]['text'])
```

**Output:**
```
ğŸ§  Reasoning: I'm looking at a straightforward question: the capital 
of France is Paris. It's a well-known fact...

ğŸ’¬ Response: The capital of France is Paris.
```

---

### **2. Reasoning Effort Levels**

```python
reasoning = {
    "effort": "low"      # RÃ¡pido, barato, menos preciso
    # ou
    "effort": "medium"   # Balanceado (padrÃ£o)
    # ou
    "effort": "high"     # Lento, caro, mais preciso
}
```

---

### **3. Summary Levels**

```python
reasoning = {
    "summary": "auto"      # Melhor disponÃ­vel para o modelo
    # ou
    "summary": "detailed"  # Mais detalhado
    # ou
    "summary": "concise"   # Mais conciso
}
```

---

### **4. Incomplete Responses**

Se o modelo fica sem tokens:

```json
{
  "status": "incomplete",
  "incomplete_details": {
    "reason": "max_output_tokens"
  },
  "output": [...]  // Pode estar vazio ou parcial
}
```

**Como lidar:**

```python
response = client.responses.create(
    model="gpt-5",
    reasoning={"effort": "medium"},
    input=[...],
    max_output_tokens=300  # Limite baixo
)

if response.status == "incomplete":
    if response.incomplete_details.reason == "max_output_tokens":
        if response.output_text:
            print("Partial:", response.output_text)
        else:
            print("Ran out during reasoning")
```

---

## ğŸ”„ **Chat Completions vs Responses**

### **Chat Completions (GPT-4 style):**

```python
# Request
response = client.chat.completions.create(
    model="gpt-4",
    messages=[
        {"role": "user", "content": "Hello"}
    ]
)

# Response
print(response.choices[0].message.content)
```

### **Responses (GPT-5 style):**

```python
# Request
response = client.responses.create(
    model="gpt-5",
    input=[
        {"role": "user", "content": "Hello"}
    ],
    reasoning={"effort": "medium", "summary": "auto"}
)

# Response
for item in response.output:
    if item.type == "reasoning":
        print("Reasoning:", item.summary[0].text)
    elif item.type == "message":
        print("Message:", item.content[0].text)
```

---

## ğŸš¨ **IMPACTO NA NOSSA IMPLEMENTAÃ‡ÃƒO**

### **SituaÃ§Ã£o Atual:**

```rust
// Nossa implementaÃ§Ã£o (Chat Completions API)
let response = client.chat_completion(
    "gpt-5",
    messages,
    options
).await?;
```

**Problemas:**
1. âŒ Estamos usando a **API errada** para GPT-5 com reasoning
2. âŒ NÃ£o conseguimos acessar **reasoning summary**
3. âŒ NÃ£o temos os parÃ¢metros corretos (`reasoning.effort`, `reasoning.summary`)
4. âŒ Response structure Ã© diferente (`output` vs `choices`)

---

### **O Que Precisamos:**

#### **OpÃ§Ã£o 1: Implementar Responses API** (Ideal)

```rust
// Nova trait para Responses API
pub trait ResponsesClient {
    async fn create_response(
        &self,
        model: &str,
        input: Vec<Message>,
        options: ResponseOptions,
    ) -> Result<Response>;
}

pub struct ResponseOptions {
    pub reasoning: Option<ReasoningConfig>,
    pub max_output_tokens: Option<u32>,
}

pub struct ReasoningConfig {
    pub effort: ReasoningEffort,  // Low, Medium, High
    pub summary: Option<SummaryLevel>,  // Auto, Detailed, Concise
}

pub struct Response {
    pub id: String,
    pub output: Vec<OutputItem>,  // Reasoning + Message items
    pub usage: Usage,
    pub status: ResponseStatus,  // Completed, Incomplete
}

pub enum OutputItem {
    Reasoning {
        id: String,
        summary: Vec<SummaryText>,
    },
    Message {
        id: String,
        content: Vec<ContentItem>,
        role: String,
    },
}
```

#### **OpÃ§Ã£o 2: Manter Chat Completions** (TemporÃ¡rio)

Continuar usando Chat Completions API para GPT-5, mas:
- âŒ Sem acesso a reasoning summary
- âŒ Menos performance (nÃ£o recomendado pela OpenAI)
- âœ… Mais simples (cÃ³digo existente funciona)

---

## ğŸ“Š **ComparaÃ§Ã£o: Mesma Request, Duas APIs**

### **Via Chat Completions (Atual):**

```bash
POST /v1/chat/completions
{
  "model": "gpt-5",
  "messages": [{"role": "user", "content": "47*89"}],
  "reasoning_effort": "high"  # â† ParÃ¢metro informal
}

Response:
{
  "choices": [{
    "message": {
      "content": "4183"  # â† SÃ³ a resposta
    }
  }],
  "usage": {
    "completion_tokens_details": {
      "reasoning_tokens": 128  # â† Tokens gastos, mas reasoning nÃ£o visÃ­vel
    }
  }
}
```

### **Via Responses (Novo):**

```bash
POST /v1/responses
{
  "model": "gpt-5",
  "input": [{"role": "user", "content": "47*89"}],
  "reasoning": {
    "effort": "high",
    "summary": "auto"  # â† ParÃ¢metro oficial
  }
}

Response:
{
  "output": [
    {
      "type": "reasoning",
      "summary": [{
        "text": "Let me break this down: 47*90=4230, 47*1=47, 4230-47=4183"
      }]
    },
    {
      "type": "message",
      "content": [{"text": "4183"}]
    }
  ],
  "usage": {
    "output_tokens_details": {
      "reasoning_tokens": 128
    }
  }
}
```

**DiferenÃ§a crÃ­tica:** Com Responses API, vocÃª **VÃŠ o summary do reasoning**!

---

## ğŸ¯ **Prioridades de ImplementaÃ§Ã£o**

### **Fase 1: Suporte BÃ¡sico a Responses API**

1. [ ] Criar client para `/v1/responses`
2. [ ] Structs: `ResponseOptions`, `ReasoningConfig`, `Response`
3. [ ] Parse do `output` array (reasoning + message items)
4. [ ] Suporte a `reasoning.effort` e `reasoning.summary`

### **Fase 2: Reasoning Summary**

1. [ ] Extrair reasoning summary do response
2. [ ] Converter para nossos `StreamEvent::Reasoning`
3. [ ] Compatibilidade com arquitetura Praxis

### **Fase 3: Streaming (se disponÃ­vel)**

1. [ ] Investigar se Responses API suporta streaming
2. [ ] Se sim, implementar SSE parsing adaptado

---

## ğŸ“ **Exemplo de Uso Futuro**

```rust
// Novo cÃ³digo (Responses API)
let response = client.create_response(
    "gpt-5",
    vec![Message::user("What is 47*89?")],
    ResponseOptions {
        reasoning: Some(ReasoningConfig {
            effort: ReasoningEffort::High,
            summary: Some(SummaryLevel::Auto),
        }),
        max_output_tokens: Some(5000),
    }
).await?;

// Processar output
for item in response.output {
    match item {
        OutputItem::Reasoning { summary, .. } => {
            for text in summary {
                println!("ğŸ§  {}", text.text);
                // Pode emitir como StreamEvent::Reasoning
            }
        }
        OutputItem::Message { content, .. } => {
            for c in content {
                println!("ğŸ’¬ {}", c.text);
                // Emitir como StreamEvent::Message
            }
        }
    }
}

println!("ğŸ“Š Reasoning tokens: {}", 
    response.usage.output_tokens_details.reasoning_tokens);
```

---

## âœ… **ConclusÃ£o**

### **Descobertas:**

1. âœ… GPT-5 tem uma **API nova** (`/v1/responses`)
2. âœ… Com essa API, vocÃª **PODE ver reasoning** (via summary)
3. âœ… Suporta `reasoning.effort` e `reasoning.summary`
4. âœ… Estrutura de response Ã© diferente (`output` array)
5. âš ï¸ Nossa implementaÃ§Ã£o atual usa a API errada

### **PrÃ³ximos Passos:**

1. **Curto prazo:** Testar Responses API com curl
2. **MÃ©dio prazo:** Implementar client Rust para Responses API
3. **Longo prazo:** Unificar ambas APIs (Chat Completions + Responses)

### **Trade-offs:**

| DecisÃ£o | PrÃ³s | Contras |
|---------|------|---------|
| **Implementar Responses API** | âœ… Acesso a reasoning<br>âœ… Melhor performance<br>âœ… Futuro-proof | âš ï¸ Mais cÃ³digo<br>âš ï¸ Nova API para aprender |
| **Manter Chat Completions** | âœ… Simples<br>âœ… CÃ³digo existente | âŒ Sem reasoning summary<br>âŒ NÃ£o recomendado OpenAI |

**RecomendaÃ§Ã£o:** Implementar Responses API para GPT-5/o1.

---

**Status:** ğŸš¨ Descoberta crÃ­tica  
**Impacto:** Alto - Arquitetura precisa adaptar  
**UrgÃªncia:** MÃ©dia - Chat Completions ainda funciona (sem reasoning summary)
