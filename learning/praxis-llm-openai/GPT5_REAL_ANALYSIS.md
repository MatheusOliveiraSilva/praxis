# GPT-5: An√°lise Real dos Dados de Streaming

## üî¨ **Teste Realizado**

**Data:** Janeiro 2025  
**Modelo:** `gpt-5-2025-08-07`  
**M√©todo:** Chamada direta √† API OpenAI com streaming habilitado

---

## üìä **Descobertas Principais**

### **1. Streaming Funciona Normalmente** ‚úÖ

GPT-5 **SUPORTA streaming** igual ao GPT-4! N√£o √© como o o1 que bloqueia streaming.

```json
// Chunk 1 (inicial)
{
  "id": "chatcmpl-CXbOXW75g3StGNZ0xvxo6y0LjcnCG",
  "object": "chat.completion.chunk",
  "created": 1762124605,
  "model": "gpt-5-2025-08-07",
  "service_tier": "default",
  "system_fingerprint": null,
  "choices": [{
    "index": 0,
    "delta": {
      "role": "assistant",
      "content": "",
      "refusal": null
    },
    "finish_reason": null
  }],
  "obfuscation": "xzrMXow"
}

// Chunks subsequentes (tokens)
{
  "choices": [{
    "delta": {
      "content": "13"
    },
    "finish_reason": null
  }],
  "obfuscation": "pxodSTB"
}

{
  "choices": [{
    "delta": {
      "content": " √ó"
    }
  }],
  "obfuscation": "KQ1cCbo"
}

{
  "choices": [{
    "delta": {
      "content": " "
    }
  }]
}

// ... mais tokens ...

// Chunk final
{
  "choices": [{
    "delta": {},
    "finish_reason": "stop"
  }],
  "obfuscation": "3bM"
}
```

---

### **2. REASONING N√ÉO √â SEPARADO** ‚ùå

**Descoberta Cr√≠tica:** Mesmo pedindo "show your work", o GPT-5 **mistura** reasoning e resposta no mesmo `delta.content`.

**Resposta completa recebida:**
```
13 √ó 7
- Break 13 into 10 + 3.
- (10 √ó 7) + (3 √ó 7) = 70 + 21 = 91.

Answer: 91
```

**Tudo veio no campo `content`!** ‚ùå N√£o h√°:
- Campo `reasoning_content`
- Campo `reasoning` no delta
- Separa√ß√£o autom√°tica de pensamento vs resposta

---

### **3. Novos Campos na Resposta**

#### **`obfuscation`** (Novo!)

Cada chunk tem um campo `obfuscation` com um valor aleat√≥rio:
```json
"obfuscation": "xzrMXow"
"obfuscation": "pxodSTB"
"obfuscation": "KQ1cCbo"
```

**Poss√≠vel prop√≥sito:**
- ü§î Anti-cache? Garante que cada chunk √© √∫nico
- ü§î Watermarking? Rastrear origem da resposta
- ü§î Seguran√ßa? Dificultar reprodu√ß√£o bit-a-bit

#### **`refusal`** (No primeiro chunk)

```json
"delta": {
  "role": "assistant",
  "content": "",
  "refusal": null
}
```

**Prop√≥sito:** Indicar se o modelo **recusou** responder (safety, policy violation).

#### **`service_tier`**

```json
"service_tier": "default"
```

Poss√≠veis valores: `default`, `premium`? Relacionado a qualidade/velocidade?

---

### **4. Mudan√ßa de Par√¢metros** ‚ö†Ô∏è

#### **`max_tokens` ‚Üí `max_completion_tokens`**

```bash
# ‚ùå N√£o funciona mais:
{
  "max_tokens": 300
}

# Erro:
{
  "error": {
    "message": "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.",
    "type": "invalid_request_error",
    "param": "max_tokens",
    "code": "unsupported_parameter"
  }
}

# ‚úÖ Funciona:
{
  "max_completion_tokens": 300
}
```

**Breaking change!** C√≥digo antigo precisa atualizar.

---

## üîÑ **Compara√ß√£o: GPT-4 vs GPT-5**

| Feature | GPT-4o | GPT-5 |
|---------|--------|-------|
| **Streaming** | ‚úÖ Sim | ‚úÖ Sim |
| **Reasoning Separado** | ‚ùå N√£o | ‚ùå N√£o |
| **Campo `obfuscation`** | ‚ùå N√£o | ‚úÖ Sim |
| **Campo `refusal`** | ‚ùå N√£o | ‚úÖ Sim |
| **`max_tokens`** | ‚úÖ Funciona | ‚ùå Deprecated |
| **`max_completion_tokens`** | ‚ùì N√£o testei | ‚úÖ Requerido |
| **Velocidade (estimada)** | R√°pido (~200ms) | R√°pido (~200ms) |
| **Tool Calling** | ‚úÖ Sim | ü§î N√£o testado ainda |

---

## üéØ **Impacto na Nossa Implementa√ß√£o**

### **Boa Not√≠cia:** ‚úÖ

Nossa implementa√ß√£o **j√° funciona** com GPT-5! A estrutura de streaming √© id√™ntica ao GPT-4.

```rust
// C√≥digo atual funciona perfeitamente!
let mut stream = client.chat_completion_stream(
    "gpt-5",  // ‚Üê S√≥ mudar o modelo
    messages,
    options
).await?;

while let Some(chunk) = stream.next().await {
    print!("{}", chunk?.content());  // ‚úÖ Funciona!
}
```

### **M√° Not√≠cia:** ‚ùå

GPT-5 **N√ÉO resolve** o problema de reasoning. Continua igual ao GPT-4:
- Reasoning misturado no `content`
- N√£o d√° pra separar automaticamente
- Precisa parsing manual ou prompting espec√≠fico

---

## üìù **Mudan√ßas Necess√°rias no C√≥digo**

### **1. Adicionar Novos Campos**

```rust
// streaming.rs
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct StreamChunk {
    pub id: String,
    pub object: String,
    pub created: i64,
    pub model: String,
    pub choices: Vec<StreamChoice>,
    
    // ‚úÖ NOVO: Campo obfuscation
    #[serde(skip_serializing_if = "Option::is_none")]
    pub obfuscation: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Delta {
    pub role: Option<String>,
    pub content: Option<String>,
    pub tool_calls: Option<Vec<ToolCallDelta>>,
    
    // ‚úÖ NOVO: Campo refusal
    #[serde(skip_serializing_if = "Option::is_none")]
    pub refusal: Option<String>,
}
```

### **2. Suportar `max_completion_tokens`**

```rust
// client.rs
#[derive(Debug, Clone, Default, Serialize, Deserialize)]
pub struct ChatOptions {
    pub temperature: Option<f32>,
    
    // ‚ùå DEPRECATED para GPT-5
    #[serde(skip_serializing_if = "Option::is_none")]
    pub max_tokens: Option<u32>,
    
    // ‚úÖ NOVO para GPT-5
    #[serde(skip_serializing_if = "Option::is_none")]
    pub max_completion_tokens: Option<u32>,
    
    pub tools: Option<Vec<Tool>>,
    pub tool_choice: Option<ToolChoice>,
}

impl ChatOptions {
    // Helper para compatibilidade
    pub fn max_output(mut self, tokens: u32) -> Self {
        // Usar novo campo se modelo for GPT-5
        self.max_completion_tokens = Some(tokens);
        self
    }
}
```

### **3. Detectar e Adaptar por Modelo**

```rust
impl OpenAIClient {
    fn build_request(&self, model: &str, ...) -> Result<Value> {
        let mut request = json!({
            "model": model,
            "messages": messages,
        });
        
        // Adaptar par√¢metros baseado no modelo
        if model.starts_with("gpt-5") {
            // GPT-5: usa max_completion_tokens
            if let Some(max_tokens) = options.max_completion_tokens {
                request["max_completion_tokens"] = json!(max_tokens);
            }
        } else {
            // GPT-4 e anteriores: usa max_tokens
            if let Some(max_tokens) = options.max_tokens {
                request["max_tokens"] = json!(max_tokens);
            }
        }
        
        Ok(request)
    }
}
```

---

## üîÆ **Conclus√µes**

### **O que Aprendemos:**

1. ‚úÖ **GPT-5 mant√©m streaming** - N√£o √© como o o1
2. ‚ùå **Reasoning ainda misturado** - N√£o h√° separa√ß√£o nativa
3. ‚ö†Ô∏è **Breaking changes na API** - `max_tokens` ‚Üí `max_completion_tokens`
4. üÜï **Novos campos** - `obfuscation`, `refusal`, `service_tier`
5. üìä **Estrutura similar** - Maior parte do c√≥digo funciona

### **Arquitetura Praxis:**

Para nossa arquitetura ter `StreamEvent::Reasoning` separado, precisamos:

**Op√ß√£o A: Prompt Engineering**
```
System: "You are a helpful assistant. When solving problems:
1. First, write your reasoning steps in a section labeled 'REASONING:'
2. Then, write your final answer in a section labeled 'ANSWER:'"
```

Depois fazer parse do texto:
```rust
if text.contains("REASONING:") {
    let parts = text.split("ANSWER:");
    let reasoning = extract_reasoning(parts[0]);
    let answer = parts[1];
    
    emit(StreamEvent::Reasoning { reasoning });
    emit(StreamEvent::Message { answer });
}
```

**Op√ß√£o B: Aceitar Limita√ß√£o**
- GPT-4 e GPT-5: Emitir tudo como `Message`
- o1: Emitir `Reasoning` + `Message` separados
- Documentar diferen√ßa

**Recomenda√ß√£o:** **Op√ß√£o B** por enquanto (mais simples e honesto).

---

## üìã **TODOs**

- [ ] Adicionar campos `obfuscation` e `refusal` nas structs
- [ ] Suportar `max_completion_tokens`
- [ ] Testar tool calling com GPT-5
- [ ] Testar multimodal (se houver)
- [ ] Atualizar README com GPT-5
- [ ] Documentar breaking changes

---

**Status:** ‚úÖ An√°lise completa com dados reais  
**Modelo Testado:** gpt-5-2025-08-07  
**Data:** Janeiro 2025
