# Pesquisa: Modelos OpenAI - Reasoning e Streaming

## ğŸ“Š **Estado Atual dos Modelos OpenAI (Nov 2024 - Jan 2025)**

---

## ğŸ¤– **1. FamÃ­lia GPT-4 (GeraÃ§Ã£o Atual)**

### **GPT-4 Turbo / GPT-4o / GPT-4o-mini**

| CaracterÃ­stica | Suporte | Detalhes |
|---------------|---------|----------|
| **Streaming** | âœ… Sim | Via `stream: true`, SSE |
| **Reasoning ExplÃ­cito** | âŒ NÃ£o | NÃ£o expÃµe "pensamento interno" separado |
| **Tool Calling** | âœ… Sim | `tool_calls` na resposta |
| **Multimodal** | âœ… Sim (gpt-4o) | VisÃ£o, imagens, texto |
| **Structured Outputs** | âœ… Sim | JSON mode, schemas |
| **Max Output Tokens** | ~4096 | gpt-4o: 4096, gpt-4-turbo: 4096 |

### **Como o "Reasoning" Funciona**

GPT-4 **NÃƒO tem campo `reasoning_content`** separado. 

**O que acontece:**
1. Modelo gera tokens **diretamente** como resposta final
2. NÃ£o hÃ¡ "pensamento interno" exposto pela API
3. Se vocÃª quer ver raciocÃ­nio, precisa **pedir no prompt**:

```json
// Prompt engineering para "forÃ§ar" reasoning
{
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful assistant. Always think step-by-step before answering. Show your work."
    },
    {
      "role": "user",
      "content": "What's 157 * 23?"
    }
  ]
}
```

**Resposta (streaming):**
```
data: {"choices":[{"delta":{"content":"Let"}}]}
data: {"choices":[{"delta":{"content":" me"}}]}
data: {"choices":[{"delta":{"content":" calculate"}}]}
data: {"choices":[{"delta":{"content":" step"}}]}
data: {"choices":[{"delta":{"content":" by"}}]}
data: {"choices":[{"delta":{"content":" step"}}]}
...
data: {"choices":[{"delta":{"content":"The answer is 3611"}}]}
```

**Problema:** Reasoning e resposta **misturados** no mesmo `content`. VocÃª nÃ£o consegue separar programaticamente.

---

## ğŸ§  **2. FamÃ­lia o1 (Reasoning Models)**

### **o1-preview / o1-mini**

LanÃ§ados em **Setembro 2024**, sÃ£o modelos **projetados para reasoning**.

| CaracterÃ­stica | Suporte | Detalhes |
|---------------|---------|----------|
| **Streaming** | âŒ **NÃƒO** | Resposta completa de uma vez |
| **Reasoning ExplÃ­cito** | âœ… **SIM** | Campo `reasoning_content` separado |
| **Tool Calling** | âŒ NÃ£o (por enquanto) | LimitaÃ§Ã£o atual |
| **Multimodal** | âŒ NÃ£o | SÃ³ texto |
| **Max Output Tokens** | ~16384 (o1-preview) | Bem maior que GPT-4 |
| **Max Reasoning Tokens** | ~32000 | Pode "pensar" muito mais |

### **Como Funciona**

#### **Request:**
```json
POST /v1/chat/completions
{
  "model": "o1-preview",
  "messages": [
    {
      "role": "user",
      "content": "Solve this complex math problem: ..."
    }
  ]
  // NÃƒO pode usar "stream": true
}
```

#### **Response (apÃ³s ~10-30 segundos):**
```json
{
  "id": "chatcmpl-o1-abc123",
  "object": "chat.completion",
  "created": 1699999999,
  "model": "o1-preview-2024-09-12",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "The answer is 42.",
        "reasoning_content": "Let me think through this step by step:\n\n1. First, I need to understand what the problem is asking...\n2. Breaking it down into smaller parts...\n3. Analyzing each component...\n...\n15. Therefore, the final answer is 42."
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 50,
    "completion_tokens": 12,
    "reasoning_tokens": 450,
    "total_tokens": 512
  }
}
```

**Campos Importantes:**
- âœ… `reasoning_content`: Pensamento interno do modelo (separado!)
- âœ… `content`: Resposta final ao usuÃ¡rio
- âœ… `reasoning_tokens`: Quantos tokens foram usados para "pensar"

### **LimitaÃ§Ãµes Atuais do o1:**

1. âŒ **Sem streaming** - VocÃª espera a resposta completa
2. âŒ **Sem tool calling** - NÃ£o pode chamar funÃ§Ãµes (ainda)
3. âŒ **Sem system messages** - SÃ³ aceita `user` e `assistant`
4. âŒ **NÃ£o aceita `temperature`** - ParÃ¢metros limitados
5. âš ï¸ **Custo alto** - `reasoning_tokens` sÃ£o cobrados

**Pricing (exemplo):**
- Input: $15 per 1M tokens
- Output: $60 per 1M tokens
- **Reasoning tokens contam como output!**

---

## ğŸ”® **3. GPT-5 / PrÃ³xima GeraÃ§Ã£o (Rumores e Expectativas)**

**Status:** NÃ£o oficialmente anunciado, mas hÃ¡ especulaÃ§Ãµes.

### **O que Sabemos (Vazamentos e Rumores):**

1. **Codinome:** "Orion" (nÃ£o confirmado)
2. **Timeline:** Possivelmente Q1-Q2 2025
3. **Treinamento:** Reportadamente em fase final

### **Expectativas Baseadas em TendÃªncias:**

#### **ProvÃ¡vel:**
- âœ… **Reasoning nativo com streaming** - Combinar o melhor de GPT-4o + o1
- âœ… **Multimodal avanÃ§ado** - VÃ­deo, Ã¡udio nativo
- âœ… **Tool calling mais robusto** - Parallel execution, retry logic
- âœ… **Maior contexto** - 128k+ tokens
- âœ… **Structured outputs melhorados** - Schemas mais complexos

#### **PossÃ­vel (EspeculaÃ§Ã£o):**
- ğŸ¤” **Reasoning em duas fases:**
  - Stream 1: `reasoning` (pensamento interno, pode ser ocultado)
  - Stream 2: `message` (resposta final ao usuÃ¡rio)
- ğŸ¤” **Controle de reasoning:**
  ```json
  {
    "reasoning_effort": "low" | "medium" | "high",
    "show_reasoning": true  // Expor ou ocultar do usuÃ¡rio
  }
  ```
- ğŸ¤” **Tool calling com reasoning:**
  - Modelo explica POR QUE estÃ¡ chamando uma tool
  - `reasoning_content`: "I need to call calculator because..."
- ğŸ¤” **Multi-step reasoning:**
  - Pode emitir mÃºltiplos blocos de reasoning
  - Pode "revisar" seu raciocÃ­nio

### **Como Poderia Ser a API (EspeculaÃ§Ã£o):**

```json
// Request
POST /v1/chat/completions
{
  "model": "gpt-5",
  "messages": [...],
  "stream": true,
  "reasoning_effort": "high",
  "stream_reasoning": true  // Novo parÃ¢metro?
}

// Response (streaming)
data: {"type":"reasoning_start"}

data: {"type":"reasoning","content":"Let me think..."}
data: {"type":"reasoning","content":" First, I should..."}

data: {"type":"reasoning_end"}

data: {"type":"message_start"}

data: {"type":"message","content":"The answer"}
data: {"type":"message","content":" is 42"}

data: {"type":"message_end"}

data: [DONE]
```

---

## ğŸ¯ **4. ComparaÃ§Ã£o: GPT-4 vs o1 vs GPT-5 (Projetado)**

| Feature | GPT-4o | o1-preview | GPT-5 (Esperado) |
|---------|--------|------------|------------------|
| **Streaming** | âœ… Sim | âŒ NÃ£o | âœ… Sim |
| **Reasoning Separado** | âŒ NÃ£o | âœ… Sim | âœ… Sim |
| **Reasoning Streaming** | âŒ NÃ£o | âŒ NÃ£o | ğŸ¤” PossÃ­vel |
| **Tool Calling** | âœ… Sim | âŒ NÃ£o | âœ… Sim (melhorado) |
| **Multimodal** | âœ… Sim | âŒ NÃ£o | âœ… Sim (avanÃ§ado) |
| **Max Reasoning Tokens** | N/A | ~32k | ğŸ¤” 50k+? |
| **LatÃªncia (primeira resposta)** | ~200ms | ~10-30s | ğŸ¤” ~1-5s? |
| **Use Case** | Geral | Problemas complexos | Ambos |

---

## ğŸ—ï¸ **5. Impacto na Nossa Arquitetura Praxis**

### **Desafio Atual:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Praxis StreamEvent (expectativa)        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Reasoning { content }  â† Token-by-token        â”‚
â”‚ Message { content }    â† Token-by-token        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†•ï¸
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   GPT-4o    â”‚  o1-preview   â”‚   GPT-5 (?)     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âœ… Streaming â”‚ âŒ Streaming  â”‚ âœ… Streaming    â”‚
â”‚ âŒ Reasoning â”‚ âœ… Reasoning  â”‚ âœ… Ambos?       â”‚
â”‚ (misturado) â”‚ (completo)    â”‚ (separado?)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **EstratÃ©gias:**

#### **OpÃ§Ã£o 1: Adapter por Modelo**

```rust
trait LLMAdapter {
    async fn execute(&self) -> Stream<StreamEvent>;
}

struct GPT4Adapter {
    // Streaming nativo, mas tudo vai como Message
    // NÃ£o tem reasoning separado
}

struct O1Adapter {
    // NÃ£o-streaming, simula chunks
    // Tem reasoning separado
}

struct GPT5Adapter {
    // (Futuro) Streaming nativo de reasoning + message
    // Parsing de dois canais
}
```

#### **OpÃ§Ã£o 2: Unified Response Type**

```rust
enum LLMResponse {
    // GPT-4: streaming simples
    StreamingContent(Stream<String>),
    
    // o1: completo com reasoning
    CompleteWithReasoning {
        reasoning: String,
        content: String,
    },
    
    // GPT-5: streaming duplo
    StreamingDual {
        reasoning_stream: Stream<String>,
        content_stream: Stream<String>,
    },
}
```

---

## ğŸ“ **6. O Que Implementar AGORA**

### **Prioridade 1: Suporte ao o1 (Atual)**

âœ… **Adicionar campos:**
```rust
pub struct ResponseMessage {
    pub content: Option<String>,
    pub reasoning_content: Option<String>,  // â† NOVO
    pub tool_calls: Option<Vec<ToolCall>>,
}

pub struct Usage {
    pub prompt_tokens: u32,
    pub completion_tokens: u32,
    pub reasoning_tokens: Option<u32>,  // â† NOVO
    pub total_tokens: u32,
}
```

âœ… **Adapter para o1:**
```rust
// Detectar modelo e escolher estratÃ©gia
match model {
    "o1-preview" | "o1-mini" => {
        // NÃ£o-streaming
        let response = client.chat_completion(...).await?;
        
        // Converter para StreamEvents (simular streaming)
        if let Some(reasoning) = response.reasoning() {
            emit(StreamEvent::Reasoning { reasoning });
        }
        emit(StreamEvent::Message { content });
    }
    _ => {
        // Streaming normal (GPT-4)
        let stream = client.chat_completion_stream(...).await?;
        // ...
    }
}
```

### **Prioridade 2: Preparar para GPT-5**

ğŸ”® **Design extensÃ­vel:**
```rust
// Nossa trait pode evoluir
trait LLMClient {
    async fn stream(
        &self,
        model: &str,
        messages: Vec<Message>,
        options: LLMOptions,
    ) -> Result<LLMResponseStream>;
}

// Stream flexÃ­vel
enum LLMResponseStream {
    Single(Stream<StreamEvent>),  // GPT-4, o1 simulado
    Dual {                        // GPT-5 futuro
        reasoning: Stream<StreamEvent::Reasoning>,
        message: Stream<StreamEvent::Message>,
    },
}
```

---

## ğŸ”¬ **7. ReferÃªncias e Fontes**

### **Oficial (OpenAI):**
- [o1 System Card](https://openai.com/index/openai-o1-system-card/)
- [Reasoning Models Guide](https://platform.openai.com/docs/guides/reasoning)
- [API Reference - Chat Completions](https://platform.openai.com/docs/api-reference/chat)

### **AnÃ¡lises e DiscussÃµes:**
- [o1 vs GPT-4: What Changed](https://community.openai.com/t/o1-reasoning-models/...)
- [Speculation on GPT-5 Features](https://www.reddit.com/r/OpenAI/...)
- [Reasoning Models Deep Dive](https://simonwillison.net/2024/Sep/12/openai-o1/)

---

## âœ… **8. ConclusÃ£o: Nossa SituaÃ§Ã£o**

### **O que estÃ¡ funcionando:**
- âœ… GPT-4 streaming (perfeito)
- âœ… Tool calling (GPT-4)
- âœ… Multimodal preparado (estrutura existe)

### **O que falta:**
- âŒ Campos `reasoning_content` e `reasoning_tokens` (o1)
- âŒ LÃ³gica para detectar modelo e escolher estratÃ©gia
- âŒ Adapter para simular streaming do o1

### **O que considerar para futuro:**
- ğŸ”® GPT-5 pode trazer streaming dual (reasoning + message)
- ğŸ”® Nossa arquitetura precisa ser flexÃ­vel
- ğŸ”® StreamEvent jÃ¡ estÃ¡ bem desenhado, mas precisamos de adapters

---

## ğŸ¯ **RecomendaÃ§Ã£o Final**

### **Curto Prazo (Agora):**
1. Adicionar campos `reasoning_content` e `reasoning_tokens`
2. MÃ©todo `response.reasoning()` para acessar
3. **NÃƒO simular streaming do o1** (enviar como bloco Ãºnico)
4. Documentar limitaÃ§Ãµes do o1 no README

### **MÃ©dio Prazo (Quando GPT-5 lanÃ§ar):**
1. Monitorar API changes
2. Implementar adapter especÃ­fico se necessÃ¡rio
3. Testar streaming dual se disponÃ­vel

### **Longo Prazo (Arquitetura Praxis):**
1. Abstrair via trait `LLMClient`
2. Cada provider (OpenAI, Anthropic, etc) implementa
3. Praxis Graph nÃ£o sabe detalhes do provider
4. Adapter layer cuida das diferenÃ§as

---

**Ãšltima AtualizaÃ§Ã£o:** Janeiro 2025  
**Status do GPT-5:** NÃ£o anunciado oficialmente  
**PrÃ³xima RevisÃ£o:** Quando houver anÃºncios oficiais
