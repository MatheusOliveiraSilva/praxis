[package]
name = "praxis-llm"
version = "0.1.1"
edition = "2021"
authors = ["Praxis Team"]
license = "MIT"
repository = "https://github.com/matheussilva/praxis"
homepage = "https://github.com/matheussilva/praxis"
documentation = "https://docs.rs/praxis-llm"
description = "Provider-agnostic LLM client with OpenAI/Azure support and streaming"
keywords = ["ai", "llm", "openai", "streaming", "async"]
categories = ["asynchronous", "api-bindings", "web-programming"]
readme = "README.md"

[dependencies]
tokio = { version = "1", features = ["full"] }
serde = { version = "1", features = ["derive"] }
serde_json = "1"
reqwest = { version = "0.12", features = ["json", "stream", "rustls-tls"], default-features = false }
anyhow = "1"
async-trait = "0.1"
futures = "0.3"
async-stream = "0.3"
chrono = { version = "0.4", features = ["serde"] }
tracing = "0.1"

[[example]]
name = "01_chat"
path = "examples/01_chat.rs"

[[example]]
name = "02_chat_streaming"
path = "examples/02_chat_streaming.rs"

[[example]]
name = "03_reasoning"
path = "examples/03_reasoning.rs"

[[example]]
name = "04_reasoning_streaming"
path = "examples/04_reasoning_streaming.rs"

